# -*- coding: utf-8 -*-
"""cnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pAicnuXZaUELSg0IZEckX_FzNEzyCoFY
"""

import torch
torch.cuda.is_available()

# imports
import matplotlib.pyplot as plt
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch

#from google.colab import drive
#drive.mount('/content/drive')

#device = 'cpu'
device=torch.device("cuda")

# Clearing Cuda cach
import gc
import os
os.environ['KMP_DUPLICATE_LIB_OK']='True'

gc.collect()

torch.cuda.empty_cache()

def get_data():
    #data_dir = '/content/drive/MyDrive/classified'
    data_dir = 'C:/Users/bl/Documents/CNNcomp472/classified'
    
    
    transform = transforms.Compose([transforms.ToTensor(),
                                    transforms.Resize((512,512)),
                                    transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))]) 

    
    original_set = datasets.ImageFolder(root=data_dir, transform=transform)  # dataset

    print(original_set.classes)
    n = len(original_set)  # total number of examples
    n_test= int(0.25*n)# take ~25% for test
    for x in range(0, 9): # rounding it to be divisible by 4
      n_test+=1
      if n_test % 4 == 0: 
        break 

    # test_set = torch.utils.data.Subset(original_set, range(n_test))  # take first 25%
    # train_set = torch.utils.data.Subset(original_set, range(n_test, n))  # take the rest


    train_set, test_set = torch.utils.data.random_split(original_set, [n-n_test, n_test])

    train = DataLoader(train_set, batch_size=32, shuffle=True)
    #test = DataLoader(test_set, batch_size=int(len(test_set)/4), shuffle=False)
    test = DataLoader(test_set, batch_size=1000, shuffle=False)

    print(len(train_set))
    print(len(test_set))
    print(n)
    

    return train, test, train_set, test_set, original_set

train, test, train_set, test_set, original_set = get_data()

class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        
        self.conv_layer1 = nn.Sequential(
            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3,stride=1, padding=1),
            nn.BatchNorm2d(32),
            nn.LeakyReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Dropout(p=0.1),
        )

        self.conv_layer2 = nn.Sequential(
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3,stride=1, padding=1),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Dropout(p=0.1),
        )

                
        self.conv_layer3 = nn.Sequential(
            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3,stride=1, padding=1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Dropout(p=0.1),
        )
        
        self.fc_layer1 = nn.Sequential(
            nn.Linear(64 * 64 * 128, 1000),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.1),
        )

        self.fc_layer2 = nn.Sequential(
            nn.Linear(1000, 1000),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.1),
        )

        self.fc_layerFinal = nn.Sequential(
            nn.Linear(1000, 4),
        )
        
    def forward(self, x):
        # conv layers
        x = self.conv_layer1(x)
        x = self.conv_layer2(x)
        x = self.conv_layer3(x)
        # flatten
        x = x.view(x.size(0), -1)
        # fc layers
        x = self.fc_layer1(x)
        x = self.fc_layer2(x)
        x = self.fc_layerFinal(x)
        return x

num_epochs = 25
num_classes = 4
learning_rate = 0.001

model = CNN()
model.to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# print accuracies at each step

total_step = len(train)
loss_list = []
acc_list = []
iterations=[]

n=0
for epoch in range(num_epochs):
  for i, (images, labels) in enumerate(train):
    # Move them to device (cuda or cpu)
    images = images.to(device)
    labels = labels.to(device)
    # Forward pass
    outputs = model(images)
    loss = criterion(outputs, labels)
    loss_list.append(loss.item())
    # Backprop and optimisation
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    # Train accuracy
    total = labels.size(0)
    _, predicted = torch.max(outputs.data, 1)
    correct = (predicted == labels).sum().item()
    acc_list.append(correct / total)

    #
    iterations.append(n)
    n+=1
    
  print('Epoch [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'
        # print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'
        .format(epoch + 1, num_epochs, loss.item(),
        (correct / total) * 100))
  
# plotting
plt.plot(iterations, loss_list)
plt.title("Training Curve (batch_size={}, lr={})".format(len(train_set), learning_rate))
plt.xlabel("Iterations")
plt.ylabel("Loss")
plt.show()
plt.plot(iterations, acc_list)
plt.title("Training Curve (batch_size={}, lr={})".format(len(train_set), learning_rate))
plt.xlabel("Iterations")
plt.ylabel("Training Accuracy")
plt.show()





# Saving the trained model and emptying the GPU memory
torch.save(model.state_dict(), 'C:/Users/bl/Desktop/cnnModelSaves/model.pth')
#del model
torch.cuda.empty_cache()

#Loading the model before testing
device = 'cpu'
model = CNN()
model.load_state_dict(torch.load('C:/Users/bl/Desktop/cnnModelSaves/model.pth'))
model = model.to(device)

# overall accuracy 
with torch.no_grad():
    correct = 0
    total = 0
    for pred_images, pred_labels in test:
        pred_images = pred_images.to(device)
        pred_labels = pred_labels.to(device)
        pred_outputs = model(pred_images)
        _, pred_predicted = torch.max(pred_outputs.data, 1)
        total += pred_labels.size(0)
        correct += (pred_predicted == pred_labels).sum().item()
    
    print('Accuracy of the network on the {} train images: {} %'.format(len(test.dataset), (correct / total)*100 ))

# print tensors

classes = original_set.classes
with torch.no_grad():
    model.eval()
    correct = 0
    total = 0
    for images, labels in test:
        images = images.to(device)
        labels = labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        print("Predicted: ",predicted, len(predicted))
        print("Expected: ",labels,len(labels))
        total += labels.size(0)

from sklearn.metrics import precision_recall_fscore_support as score
from sklearn.metrics import classification_report

eval_labels = pred_labels.detach().cpu()
eval_predictions = pred_predicted.detach().cpu()


print(classification_report(eval_labels, eval_predictions))

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from matplotlib.pyplot import figure

figure(figsize=(80, 60), dpi=80)
cm = confusion_matrix(eval_labels, eval_predictions)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=original_set.classes)
disp.plot()

plt.show()